{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import mlflow\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(batch_size: int) -> Tuple[Any]:\n",
    "    # Start of load time.\n",
    "    start_time = time()\n",
    "\n",
    "    # Define a transform to normalize the data\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                                ])\n",
    "\n",
    "    # Download and load the training data\n",
    "    train_dataset = datasets.MNIST('./mnistdata', download=True, train=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST('./mnistdata', download=True, train=False, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader, test_loader, len(train_dataset), len(test_dataset), (time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_sizes: List[int], output_size: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lin1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.lin2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.lin3 = nn.Linear(hidden_sizes[1], output_size)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.output_activation = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.lin1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.lin2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.lin3(out)\n",
    "        out = self.output_activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: MNISTModel, loader: DataLoader, params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    start_time = time()\n",
    "    loss_func = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=params['lr'], momentum=params['momentum'])\n",
    "    training_metrics = {}\n",
    "    for epoch in range(params['epochs']):\n",
    "        total_loss = 0\n",
    "        for images, labels in loader:\n",
    "            # Flatten MNIST images into a 784 long vector.\n",
    "            images = images.view(images.shape[0], -1)\n",
    "        \n",
    "            # Training pass\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(images)\n",
    "            loss = loss_func(output, labels)\n",
    "            \n",
    "            # This is where the model learns by backpropagating\n",
    "            loss.backward()\n",
    "            \n",
    "            # And optimizes its weights here\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            mlflow.log_metric('training_loss', total_loss/len(loader), epoch+1)\n",
    "            print(\"Epoch {} - Training loss: {}\".format(epoch+1, total_loss/len(loader)))\n",
    "\n",
    "    training_time_sec = (time()-start_time)\n",
    "    training_metrics['training_time_sec'] = training_time_sec\n",
    "    print(\"\\nTraining Time (in seconds) =\",training_time_sec)\n",
    "    return training_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model: MNISTModel, loader: DataLoader) -> Dict[str, Any]:\n",
    "    correct_count, total_count = 0, 0\n",
    "    for images,labels in loader:\n",
    "        for i in range(len(labels)):\n",
    "            img = images[i].view(1, 784)\n",
    "            # Turn off gradients to speed up this part\n",
    "            with torch.no_grad():\n",
    "                logps = model(img)\n",
    "\n",
    "            # Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "            ps = torch.exp(logps)\n",
    "            probab = list(ps.numpy()[0])\n",
    "            pred_label = probab.index(max(probab))\n",
    "            true_label = labels.numpy()[i]\n",
    "            if(true_label == pred_label):\n",
    "                correct_count += 1\n",
    "            total_count += 1\n",
    "    \n",
    "    testing_metrics = {\n",
    "        'incorrect_count': total_count-correct_count,\n",
    "        'correct_count': correct_count,\n",
    "        'accuracy': (correct_count/total_count)\n",
    "    }\n",
    "    print(\"Number Of Images Tested =\", total_count)\n",
    "    print(\"\\nModel Accuracy =\", (correct_count/total_count))\n",
    "    return testing_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.4619285132187897\n",
      "Epoch 2 - Training loss: 0.20306115457093094\n",
      "Epoch 3 - Training loss: 0.14526802751380624\n",
      "Epoch 4 - Training loss: 0.11487165860720534\n",
      "Epoch 5 - Training loss: 0.09709349530897955\n",
      "Epoch 6 - Training loss: 0.08283366170971951\n",
      "Epoch 7 - Training loss: 0.07259985393554623\n",
      "Epoch 8 - Training loss: 0.06504740772236671\n",
      "Epoch 9 - Training loss: 0.057733049282113084\n",
      "Epoch 10 - Training loss: 0.0500828753436989\n",
      "Epoch 11 - Training loss: 0.04607762152496388\n",
      "Epoch 12 - Training loss: 0.04253278530117418\n",
      "Epoch 13 - Training loss: 0.03821447054380928\n",
      "Epoch 14 - Training loss: 0.03408039376516439\n",
      "Epoch 15 - Training loss: 0.030731470181310335\n",
      "Epoch 16 - Training loss: 0.02761096904717777\n",
      "Epoch 17 - Training loss: 0.025361674866654714\n",
      "Epoch 18 - Training loss: 0.02447121817480412\n",
      "Epoch 19 - Training loss: 0.021156636417609703\n",
      "Epoch 20 - Training loss: 0.01969279076063895\n",
      "Epoch 21 - Training loss: 0.016983427853018953\n",
      "Epoch 22 - Training loss: 0.015015803878061545\n",
      "Epoch 23 - Training loss: 0.012715912695177573\n",
      "Epoch 24 - Training loss: 0.011392286963505062\n",
      "Epoch 25 - Training loss: 0.011942075991364598\n",
      "Epoch 26 - Training loss: 0.010427751705968076\n",
      "Epoch 27 - Training loss: 0.00802282343248196\n",
      "Epoch 28 - Training loss: 0.009474762845241754\n",
      "Epoch 29 - Training loss: 0.006012360423343564\n",
      "Epoch 30 - Training loss: 0.005649626712022552\n",
      "Epoch 31 - Training loss: 0.004806690724101756\n",
      "Epoch 32 - Training loss: 0.003530642385724242\n",
      "Epoch 33 - Training loss: 0.0023895172153895004\n",
      "Epoch 34 - Training loss: 0.0019066535328333021\n",
      "Epoch 35 - Training loss: 0.0017357691530473383\n",
      "\n",
      "Training Time (in seconds) = 92.10235285758972\n",
      "Number Of Images Tested = 10000\n",
      "\n",
      "Model Accuracy = 0.9804\n"
     ]
    }
   ],
   "source": [
    "# Setup parameters\n",
    "params = {\n",
    "    'batch_size': 64,\n",
    "    'epochs': 35,\n",
    "    'input_size': 784,\n",
    "    'hidden_sizes': [128, 64],\n",
    "    'lr': 0.035,\n",
    "    'momentum': 0.5,\n",
    "    'output_size': 10\n",
    "    }\n",
    "\n",
    "# Setup mlflow to point to our server.\n",
    "run_name = f'Learning rate={params[\"lr\"]}'\n",
    "mlflow.set_tracking_uri('http://localhost:5001/')\n",
    "mlflow.set_experiment('MNIST 3-layer network')\n",
    "mlflow.start_run(run_name=run_name)\n",
    "\n",
    "# Log parameters\n",
    "mlflow.log_params(params)\n",
    "\n",
    "# Load the data and log loading metrics.\n",
    "train_loader, test_loader, train_size, test_size, load_time_sec = load_images(params['batch_size'])\n",
    "mlflow.log_metric('train_size', train_size)\n",
    "mlflow.log_metric('test_size', test_size)\n",
    "mlflow.log_metric('load_time_sec', load_time_sec)\n",
    "\n",
    "# Train the model and log training metrics.\n",
    "model = MNISTModel(params['input_size'], params['hidden_sizes'], params['output_size'])\n",
    "training_metrics = train_model(model, train_loader, params)\n",
    "mlflow.log_metrics(training_metrics)\n",
    "\n",
    "# Test the model and log the accuracy as a metric.\n",
    "testing_metrics = test_model(model, test_loader)\n",
    "mlflow.log_metrics(testing_metrics)\n",
    "\n",
    "# Log the raw data and the trained model as artifacts.\n",
    "mlflow.log_artifacts('./mnistdata', artifact_path='mnistdata')\n",
    "mlflow.pytorch.log_model(model, artifact_path='mnistmodel')\n",
    "\n",
    "# End the run\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
