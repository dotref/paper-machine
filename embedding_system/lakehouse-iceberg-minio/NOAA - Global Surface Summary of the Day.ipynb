{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144a7f5d-61df-49f4-8e65-8790d6fec91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import tarfile\n",
    "from time import time\n",
    "from typing import List\n",
    "\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "bucket_name = 'lake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7060a77a-9ac5-4495-b0aa-031a4dc6f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tar_to_df(bucket_name: str, object_name: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    This function will take a tarfile reference in MinIO and do the following:\n",
    "        - unzip the tarfile\n",
    "        - turn the data into a single DataFrame object\n",
    "    '''\n",
    "    logger = logging.getLogger('gsod_logger')\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Temp file to use for processing the tar files.\n",
    "    temp_file_name = 'temp.tar.gz'\n",
    "    # Load the credentials and connection information.\n",
    "    with open('credentials.json') as f:\n",
    "        credentials = json.load(f)\n",
    "\n",
    "    # Get data of an object.\n",
    "    try:\n",
    "        # Create client with access and secret key\n",
    "        client = Minio(credentials['url'],  # host.docker.internal\n",
    "                    credentials['accessKey'],  \n",
    "                    credentials['secretKey'], \n",
    "                    secure=False)\n",
    "        \n",
    "        object_info = client.fget_object(bucket_name, object_name, temp_file_name)\n",
    "\n",
    "        Row = namedtuple('Row', ('station', 'date', 'latitude', 'longitude', 'elevation', 'name', 'temp', 'temp_attributes', 'dewp',\n",
    "                                'dewp_attributes', 'slp', 'SLP_attributes', 'stp', 'stp_attributes', 'visib', 'visib_attributes',\n",
    "                                'wdsp', 'wdsp_attributes', 'mxspd', 'gust', 'max', 'max_attributes', 'min', 'min_attributes', 'prcp',\n",
    "                                'prcp_attributes', 'sndp', 'frshtt'))\n",
    "        \n",
    "        # Columns of interest and their data types.\n",
    "        dtypes={\n",
    "            'station': 'string',\n",
    "            'date': 'datetime64[ns]',\n",
    "            'latitude': 'float64',\n",
    "            'longitude': 'float64',\n",
    "            'name': 'string',\n",
    "            'temp': 'float64'\n",
    "        }\n",
    "\n",
    "        tar = tarfile.open(temp_file_name, 'r:gz')\n",
    "        all_rows = []\n",
    "        for member in tar.getmembers():\n",
    "            member_handle = tar.extractfile(member)\n",
    "            byte_data = member_handle.read()\n",
    "            decoded_string = byte_data.decode()\n",
    "            lines = decoded_string.splitlines()\n",
    "            reader = csv.reader(lines, delimiter=',')\n",
    "\n",
    "            # Get all the rows in the member. Skip the header.\n",
    "            _ = next(reader)\n",
    "            file_rows = [Row(*l) for l in reader]\n",
    "            all_rows += file_rows\n",
    "\n",
    "        df = pd.DataFrame.from_records(all_rows, columns=Row._fields)\n",
    "        \n",
    "        df = df[list(dtypes.keys())]\n",
    "        for c in df.columns:\n",
    "            if dtypes[c] == 'float64': df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "        df = df.astype(dtype=dtypes)\n",
    "\n",
    "    except S3Error as s3_err:\n",
    "        logger.error(f'S3 Error occurred: {s3_err}.')\n",
    "        raise s3_err\n",
    "    except Exception as err:\n",
    "        logger.error(f'Error occurred: {err}.')\n",
    "        raise err\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d7fda1-c858-4bb9-8f76-f64844dc299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_list(bucket_name: str, prefix: str) -> List[str]:\n",
    "    '''\n",
    "    Gets a list of objects from a bucket.\n",
    "    '''\n",
    "    logger = logging.getLogger('gsod_logger')\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Load the credentials and connection information.\n",
    "    with open('credentials.json') as f:\n",
    "        credentials = json.load(f)\n",
    "\n",
    "    # Get data of an object.\n",
    "    try:\n",
    "        # Create client with access and secret key\n",
    "        client = Minio(credentials['url'],  # host.docker.internal\n",
    "                    credentials['accessKey'],  \n",
    "                    credentials['secretKey'], \n",
    "                    secure=False)\n",
    "\n",
    "        object_list = []\n",
    "        objects = client.list_objects(bucket_name, prefix=prefix, recursive=True)\n",
    "        for obj in objects:\n",
    "            object_list.append(obj.object_name)\n",
    "    except S3Error as s3_err:\n",
    "        logger.error(f'S3 Error occurred: {s3_err}.')\n",
    "        raise s3_err\n",
    "    except Exception as err:\n",
    "        logger.error(f'Error occurred: {err}.')\n",
    "        raise err\n",
    "\n",
    "    return object_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f2c448-6ae5-47d1-b133-c144e26ac3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Jupyter').getOrCreate()\n",
    "\n",
    "objects = get_object_list(bucket_name, 'noaa/gsod')\n",
    "\n",
    "for obj in reversed(objects):\n",
    "    print(obj)\n",
    "    df = tar_to_df(bucket_name, obj)\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, 'temp.parquet')\n",
    "    \n",
    "    df = spark.read.parquet('temp.parquet')\n",
    "    df.write.mode('append').saveAsTable('noaa.gsod')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd972cb4-e704-44b5-9672-7759ca3e367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = 'noaa/gsod/2011.tar.gz'\n",
    "df = tar_to_df(bucket_name, obj)\n",
    "table = pa.Table.from_pandas(df)\n",
    "pq.write_table(table, 'temp.parquet')\n",
    "    \n",
    "df = spark.read.parquet('temp.parquet')\n",
    "df.write.mode('append').saveAsTable('noaa.gsod')\n",
    "print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db3ffc4-b27a-4c51-9e8e-da2b45d954fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
